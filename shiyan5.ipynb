{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、加载 Qwen1.5-0.5B（远程加载）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型 Qwen/Qwen1.5-0.5B 加载完成\n",
      "模型参数量: 463.99M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import json\n",
    "from datasets import Dataset\n",
    "import accelerate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model_name = \"Qwen/Qwen1.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\")\n",
    "print(f\"模型 {model_name} 加载完成\")\n",
    "print(f\"模型参数量: {model.num_parameters()/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 二、Prompt 设计问题\n",
    "\n",
    "### 问题 1：Prompt 结构设计\n",
    "\n",
    "分别针对**文本摘要**、**问题回答**、**角色扮演**，设计3种不同类型的Prompt。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义三种任务的Prompt模板\n",
    "prompt_templates = {\n",
    "    # 任务A：文本摘要（改进：要求明确字数）\n",
    "    \"summarization\": \"\"\"请用50字左右概括以下内容，需包含关键数据：\n",
    "{input}\n",
    "精简摘要：\"\"\",\n",
    "    \n",
    "    # 任务B：问题回答（改进：要求完整回答+结构化输出）\n",
    "    \"qa\": \"\"\"请作为专业健康顾问完整回答以下问题，至少列出5个要点：\n",
    "问题：{input}\n",
    "完整回答：\"\"\",\n",
    "    \n",
    "    # 任务C：角色扮演（改进：明确专业领域）\n",
    "    \"role_playing\": \"\"\"你是一名三甲医院心内科主任医师，请用专业但易懂的方式回答：\n",
    "患者问：{input}\n",
    "医生答：\"\"\"\n",
    "}\n",
    "def generate_response(prompt, max_length=300):\n",
    "    inputs = tokenizer(prompt, \n",
    "                      return_tensors=\"pt\",\n",
    "                      truncation=True,\n",
    "                      max_length=512)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_length,  # 增加生成长度\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        num_beams=3,               # 束搜索提高质量\n",
    "        early_stopping=True,       # 自然停止\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=2     # 避免重复\n",
    "    )\n",
    "    \n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return full_response[len(prompt):]  # 返回纯回答内容\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 问题 2：Prompt 区别分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 不同Prompt效果测试 ===\n",
      "=== 改进后的Prompt效果测试 ===\n",
      "\n",
      "【summarization】Prompt结构：\n",
      "请用50字左右概括以下内容，需包含关键数据：\n",
      "\n",
      "最新发表在《美国心脏病学杂志》上的一项长期追踪研究表明，每日保持30分钟的中等强度体育锻炼，如快走、游泳或骑自行车，能够显著降低心血管疾病发病风险约35-40%。这项研究跟踪了超过12,000名年龄在40-75岁之间的参与者，历时15年。\n",
      "\n",
      "研究负责人、哈佛医学院的约翰·史密斯教授指出，规律运动不仅对心脏健康有益，还能有效缓解焦虑和抑郁症状。数据分析显示，坚持锻炼的参与者心理健康评分平均提高了22%，睡眠质量也有明显改善。\n",
      "\n",
      "此外，研究还发现，运动带来的健康效益具有累积效应。每周累计150分钟的运动时间，分散在3-5天内完成，就能达到最佳效果。史密斯教授特别强调，对于久坐不动的办公室人群，即使只是每隔一小时站起来活动5分钟，也能带来明显的健康改善。\n",
      "\n",
      "这项研究的重要意义在于，它证实了适度运动比高强度训练更适合普通人群。研究团队建议，公共卫生政策应该更加重视推广这种简单易行的健康生活方式，而不是过度强调需要专业指导的高强度训练。\n",
      "\n",
      "精简摘要：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "模型回答：\n",
      "最新研究证明，每天保持半小时的有氧运动可以降低心脏病和中风的风险。该研究追踪了近1.2万名参与者，并发现每周至少锻炼3次或5次的人群，其心理健康和睡眠表现更好。\n",
      "==================================================\n",
      "\n",
      "【qa】Prompt结构：\n",
      "请作为专业健康顾问完整回答以下问题，至少列出5个要点：\n",
      "问题：如何保持心脏健康？\n",
      "完整回答：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "模型回答：\n",
      " 1. 饮食健康：保持均衡的饮食，摄入足够的蛋白质、碳水化合物、脂肪、维生素和矿物质。\n",
      "2. 锻炼身体：每周至少进行150分钟的中等强度有氧运动，如快走、慢跑、游泳、骑自行车等。\n",
      "3. 控制体重：过重或肥胖会增加心脏疾病的风险，因此需要控制体重。\n",
      "4. 戒烟限酒：吸烟和饮酒会损害心脏，应尽量避免。\n",
      "5. 定期体检：定期进行心电图、血压、血脂、血糖等检查，及时发现和治疗潜在的健康问题。\n",
      "==================================================\n",
      "\n",
      "【role_playing】Prompt结构：\n",
      "你是一名三甲医院心内科主任医师，请用专业但易懂的方式回答：\n",
      "患者问：如何保持心脏健康？\n",
      "医生答：\n",
      "\n",
      "模型回答：\n",
      "保持健康的心脏需要良好的生活习惯和饮食习惯。首先，要保证充足的睡眠，每天睡眠时间不少于7小时。其次，饮食要均衡，多吃新鲜蔬菜和水果，少吃高脂肪、高热量的食物，如油炸食品、甜食等。此外，还要戒烟戒酒，避免过度劳累和情绪波动。最后，定期进行体检和心电图检查，及时发现和治疗心脏疾病。\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "test_input = \"\"\"\n",
    "最新发表在《美国心脏病学杂志》上的一项长期追踪研究表明，每日保持30分钟的中等强度体育锻炼，如快走、游泳或骑自行车，能够显著降低心血管疾病发病风险约35-40%。这项研究跟踪了超过12,000名年龄在40-75岁之间的参与者，历时15年。\n",
    "\n",
    "研究负责人、哈佛医学院的约翰·史密斯教授指出，规律运动不仅对心脏健康有益，还能有效缓解焦虑和抑郁症状。数据分析显示，坚持锻炼的参与者心理健康评分平均提高了22%，睡眠质量也有明显改善。\n",
    "\n",
    "此外，研究还发现，运动带来的健康效益具有累积效应。每周累计150分钟的运动时间，分散在3-5天内完成，就能达到最佳效果。史密斯教授特别强调，对于久坐不动的办公室人群，即使只是每隔一小时站起来活动5分钟，也能带来明显的健康改善。\n",
    "\n",
    "这项研究的重要意义在于，它证实了适度运动比高强度训练更适合普通人群。研究团队建议，公共卫生政策应该更加重视推广这种简单易行的健康生活方式，而不是过度强调需要专业指导的高强度训练。\n",
    "\"\"\"\n",
    "print(\"\\n=== 不同Prompt效果测试 ===\")\n",
    "test_question = \"如何保持心脏健康？\"\n",
    "test_article = \"\"\"\n",
    "最新发表在《美国心脏病学杂志》上的一项长期追踪研究表明，每日保持30分钟的中等强度体育锻炼，如快走、游泳或骑自行车，能够显著降低心血管疾病发病风险约35-40%。这项研究跟踪了超过12,000名年龄在40-75岁之间的参与者，历时15年。\n",
    "\n",
    "研究负责人、哈佛医学院的约翰·史密斯教授指出，规律运动不仅对心脏健康有益，还能有效缓解焦虑和抑郁症状。数据分析显示，坚持锻炼的参与者心理健康评分平均提高了22%，睡眠质量也有明显改善。\n",
    "\n",
    "此外，研究还发现，运动带来的健康效益具有累积效应。每周累计150分钟的运动时间，分散在3-5天内完成，就能达到最佳效果。史密斯教授特别强调，对于久坐不动的办公室人群，即使只是每隔一小时站起来活动5分钟，也能带来明显的健康改善。\n",
    "\n",
    "这项研究的重要意义在于，它证实了适度运动比高强度训练更适合普通人群。研究团队建议，公共卫生政策应该更加重视推广这种简单易行的健康生活方式，而不是过度强调需要专业指导的高强度训练。\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== 改进后的Prompt效果测试 ===\")\n",
    "for task_name, template in prompt_templates.items():\n",
    "    input_content = test_question if task_name != \"summarization\" else test_article\n",
    "    prompt = template.format(input=input_content)\n",
    "    \n",
    "    print(f\"\\n【{task_name}】Prompt结构：\\n{prompt}\")\n",
    "    \n",
    "    response = generate_response(prompt)\n",
    "    \n",
    "    # 完整性检查（新增功能）\n",
    "    if not response.endswith((\"。\", \"！\", \"？\")):\n",
    "        print(\"⚠️ 检测到回答不完整，尝试补充...\")\n",
    "        continuation = generate_response(prompt + response, max_length=100)\n",
    "        response += continuation\n",
    "    \n",
    "    print(f\"\\n模型回答：\\n{response}\\n{'='*50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 任务 2：探索 Prompt 影响\n",
    "\n",
    "#### 问题 3：最短 Prompt\n",
    "找到最短仍然能理解的Prompt："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【问答】Prompt: '光速？'\n",
      "   → （　　）\n",
      "A. 10m/s\n",
      "B. 约11.2km/s（1m=1×18km）\n",
      "C. 大约1光年\n",
      "D. 小于1km/h\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【摘要】Prompt: '概括：光速'\n",
      "   → 是宇宙中最快的速度，是人类目前所能观测到的最快速度。光的速度是每秒299792458米，约为每秒钟300亿公里。在宇宙中的任何地方，光都可以以\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【翻译】Prompt: '英→中：hello'\n",
      "   → →hello\n",
      "故答案为：Hello．\n",
      "解析：你好，你好．\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【代码】Prompt: 'py打印hello'\n",
      "   → world\n",
      "print(\"hello\")\n",
      "print(\"\")\n",
      "print(\"\\n\")\n",
      "# 1. 2.3.4.5.6.7.8.9.10.\n",
      "\n",
      "【极简】Prompt: '光速'\n",
      "   → 为3×108m/s，光在真空中传播的速度为c=3.0× 1 0 8 m/s，则光从地球到月球需要的时间为（　　）\n",
      "A. 2.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen1.5-0.5B\", device_map=\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-0.5B\")\n",
    "\n",
    "# 最小化Prompt测试集\n",
    "minimal_prompts = [\n",
    "    (\"问答\", \"光速？\"),          # 3字符\n",
    "    (\"摘要\", \"概括：光速\"),      # 5字符 \n",
    "    (\"翻译\", \"英→中：hello\"),    # 6字符\n",
    "    (\"代码\", \"py打印hello\"),     # 7字符\n",
    "    (\"极简\", \"光速\")            # 2字符（测试下限）\n",
    "]\n",
    "\n",
    "for task, prompt in minimal_prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"【{task}】Prompt: '{prompt}'\\n   → {response[len(prompt):].strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 问题 4：改变 Prompt 语气\n",
    "如增加“请详细回答”，对输出有何影响？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 修正版语气影响测试 ===\n",
      "\n",
      "🔹 简洁版 Prompt: '请用1句话简单回答：水的化学分子式是什么?'\n",
      "💡 模型回答:\n",
      "水的分子结构是H2O，由两个氢原子和一个氧原子组成。\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "\n",
      "🔹 专业版 Prompt: '请用专业术语分点解释：水的化学分子式是什么?\n",
      "1.'\n",
      "💡 模型回答:\n",
      "水分子由两个氢原子和一个氧原子组成。\n",
      "2. 在水分子中，氢和氧的原子个数比为1:2，即1个氢分子和2个氧分子。\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "\n",
      "🔹 通俗版 Prompt: '用小学生能听懂的话回答：水的化学分子式是什么?'\n",
      "💡 模型回答:\n",
      "水分子的结构式是怎样的?\n",
      "\n",
      "水是H2O,分子中H原子和O原子以共价键结合,而O和H之间以非极性键相连,所以水是一种双原子分子,其结构简式为H-O-O-H.\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "\n",
      "🔹 严谨版 Prompt: '水的化学分子式是什么?（需提供3个科学依据）'\n",
      "💡 模型回答:\n",
      "水的分子结构是H2O,分子量是18,水分子中氢原子和氧原子的个数比是2:1,所以水是双原子分子,化学式是 H2 O, 每个水原子的质量是 1.80614×10-26kg,每个水离子的质量为 3.2035×  2 6  kg, 一个水气分子的质量 0.001278×22.4  g,一个氢离子质量 9.11×3 4 g  求水在水溶液中电离出的离子的物质的量浓度,\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "\n",
      "🔹 角色版 Prompt: '你是有30年经验的化学教授，正在给大学生授课。学生问：水的化学分子式是什么?'\n",
      "💡 模型回答:\n",
      "教授回答：是H2O。你认为教授的回答是（ ）。\n",
      "A. 错误的，因为水分子中没有氢原子\n",
      "B. 正确的，在水溶液中，氢离子和氢氧根离子以离子键结合\n",
      "C. 有误，水是共价化合物\n",
      "D. 不确定，要看水是什么状态\n",
      "答案：B\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n"
     ]
    }
   ],
   "source": [
    "tone_variations = [\n",
    "    {\"name\": \"简洁版\", \"prompt\": \"请用1句话简单回答：{input}\", \"params\": {\"temperature\": 0.3}},\n",
    "    {\"name\": \"专业版\", \"prompt\": \"请用专业术语分点解释：{input}\\n1.\", \"params\": {\"temperature\": 0.5, \"num_beams\": 3}},\n",
    "    {\"name\": \"通俗版\", \"prompt\": \"用小学生能听懂的话回答：{input}\", \"params\": {\"top_k\": 40}},\n",
    "    {\"name\": \"严谨版\", \"prompt\": \"{input}（需提供3个科学依据）\", \"params\": {\"do_sample\": False}},\n",
    "    {\"name\": \"角色版\", \"prompt\": \"你是有30年经验的化学教授，正在给大学生授课。学生问：{input}\", \"params\": {\"num_return_sequences\": 1}}\n",
    "]\n",
    "\n",
    "print(\"\\n=== 修正版语气影响测试 ===\")\n",
    "for tone in tone_variations:\n",
    "    prompt = tone[\"prompt\"].format(input=\"水的化学分子式是什么?\")\n",
    "    print(f\"\\n🔹 {tone['name']} Prompt: '{prompt}'\")\n",
    "    \n",
    "    # 修正的输入处理\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # 动态生成参数\n",
    "    base_params = {\n",
    "        **inputs,\n",
    "        \"max_new_tokens\": 150,\n",
    "        \"early_stopping\": True,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"no_repeat_ngram_size\": 2,  # 防止重复\n",
    "        \"num_beams\": 1\n",
    "    }\n",
    "    params = {**base_params, **tone[\"params\"]}\n",
    "    \n",
    "    try:\n",
    "        outputs = model.generate(**params)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 提取生成部分（去除Prompt）\n",
    "        generated = response[len(prompt):].strip()\n",
    "        \n",
    "        # 二次校验（防止截断）\n",
    "        if not generated or len(generated) < 3:\n",
    "            raise ValueError(\"生成内容过短\")\n",
    "            \n",
    "        print(f\"💡 模型回答:\\n{generated}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 生成失败: {str(e)}\")\n",
    "    \n",
    "    print(\"━\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 三、LoRA 微调问题\n",
    "\n",
    "### LoRA 相关库与配置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 加载Tokenizer和模型\n",
    "model_name = \"Qwen/Qwen1.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载训练数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 192/192 [00:00<00:00, 2335.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 假设你的数据文件为 train.jsonl，格式为每行一个{\"input\": \"...\", \"output\": \"...\"}\n",
    "data = load_dataset(\"json\", data_files=\"train_data.json\")[\"train\"]\n",
    "\n",
    "def preprocess(example):\n",
    "    prompt = example[\"input\"]\n",
    "    answer = example[\"output\"]\n",
    "    full_prompt = prompt + tokenizer.eos_token + answer + tokenizer.eos_token\n",
    "    tokenized = tokenizer(\n",
    "        full_prompt, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\",  # 强制所有样本统一为最大长度\n",
    "        max_length=512         # 根据模型最大支持长度调整\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # 确保 labels 与 input_ids 对齐\n",
    "    return tokenized\n",
    "\n",
    "tokenized_data = data.map(\n",
    "    preprocess,\n",
    "    remove_columns=data.column_names  # 删除原始 input/output 列，避免干扰\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目录 'lora-5epoch' 存在: False\n",
      "✗ 目录写入失败: [Errno 2] No such file or directory: 'lora-5epoch\\\\test.txt'\n"
     ]
    }
   ],
   "source": [
    "# 检查目录权限\n",
    "print(f\"目录 'lora-5epoch' 存在: {os.path.exists('lora-5epoch')}\")\n",
    "try:\n",
    "    # 尝试创建测试文件\n",
    "    with open(os.path.join('lora-5epoch', 'test.txt'), 'w') as f:\n",
    "        f.write('test')\n",
    "    print(\"✓ 目录可写入\")\n",
    "    # 清理测试文件\n",
    "    os.remove(os.path.join('lora-5epoch', 'test.txt'))\n",
    "except Exception as e:\n",
    "    print(f\"✗ 目录写入失败: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA 微调训练代码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 46:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.048100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.934600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.739500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.155400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.772700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.117200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.306800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.645900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.408300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.195400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.230200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.422200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.499800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.561800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.066800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.590600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.010900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.978700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.496800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.727400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.766800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.378900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.294900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.756100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.853000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.708500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.883400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.300700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.622200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.637200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.428400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.989900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.323800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.840400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.915400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.480100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.736400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.447300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.291700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.044500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.078300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.467900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.652400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.634700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.769300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.950900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.731000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.125400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.758300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.368200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.549600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.538500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.995500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.972100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.792200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.729800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.931200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.822300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.669200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.509300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.743500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.279300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.673300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.085400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.201800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.091500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.975900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.154200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.451400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.745300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.838700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.487500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.483000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.175200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.812700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.878300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.610400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.852300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.649400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.936500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.733500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.054600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.500700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.521100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.379100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.907500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.832800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.777800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.867000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.780200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.800600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.701700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.773200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.676100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.780800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>1.508700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.631600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.951400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.939600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.804700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>1.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.967700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>1.669800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.992700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.790900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>1.232600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.749000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.830500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.811300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.707800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.524200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.736700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.723500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.657900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.366200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.849700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.738400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>1.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>1.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.829400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.838400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.784500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.710900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.918100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>1.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>1.488300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.666900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.826200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.484000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.737800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>1.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.682200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.931500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.450200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.328700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>1.444900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>1.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.956100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.967500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.796800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.871400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.605700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>1.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.707600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>1.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.541200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.362400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.588100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.732600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.644600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.626400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.631700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.646000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.565100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.921300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.696100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.507500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.569900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.988700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.962400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.609700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>1.218200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.309200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.538600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>1.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.375900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.985500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.529100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.584000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.467100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.544200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.673800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.712800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.877800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>1.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.998900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.762300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.372500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.405200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>1.108700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>1.436400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.665800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.827200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.669400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.962400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.486500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.996300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.042200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.966600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.657500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>1.494900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.818800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>1.153900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.631900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.863200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.712000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\pytorch2.3.0\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen1.5-0.5B/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000022F1E7B8F10>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: feff0af3-b0d0-4073-a947-c225475e55a1)') - silently ignoring the lookup for the file config.json in Qwen/Qwen1.5-0.5B.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA 微调完成, 权重保存在 lora-5epoch\n"
     ]
    }
   ],
   "source": [
    "def train_lora(epochs, output_dir):\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.1\n",
    "    )\n",
    "    model = get_peft_model(base_model, config)\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False  # 设置为 False 以适配 Causal LM\n",
    ")\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=4,\n",
    "        save_strategy=\"no\",\n",
    "        remove_unused_columns=False,\n",
    "        logging_steps=1,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=False,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_data,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    trainer.train()\n",
    "    model.save_pretrained(output_dir)\n",
    "    print(f\"LoRA 微调完成, 权重保存在 {output_dir}\")\n",
    "\n",
    "def infer(model, prompt):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    assert isinstance(prompt, str), \"Prompt must be a string!\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "def plot_answer_comparison(question, answers, labels, filename):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.title(f\"问题：{question}\")\n",
    "    plt.axis(\"off\")\n",
    "    y0 = 1.0\n",
    "    for ans, label in zip(answers, labels):\n",
    "        plt.text(0, y0, f\"{label}:\\n{ans}\", fontsize=12, verticalalignment=\"top\")\n",
    "        y0 -= 0.35\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"已保存对比图：{filename}\")\n",
    "if __name__ == \"__main__\":\n",
    "    # -------------------\n",
    "    # 训练阶段\n",
    "    # -------------------\n",
    "    # 训练5轮和10轮的LoRA模型\n",
    "    if not os.path.exists(\"lora-5epoch\"):\n",
    "        train_lora(5, \"lora-5epoch\")\n",
    "    if not os.path.exists(\"lora-10epoch\"):\n",
    "        train_lora(10, \"lora-10epoch\")\n",
    "\n",
    "    # -------------------\n",
    "    # 推理与结果分析\n",
    "    # -------------------\n",
    "    # 选择一个问题进行分析（可自行替换为实际问题）\n",
    "    test_question = \"请简述LoRA的基本思想。\"\n",
    "\n",
    "    # 1. 原始Qwen模型回答\n",
    "    original_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    orig_answer = infer(original_model, test_question)\n",
    "\n",
    "    # 2. LoRA 微调模型（5轮）\n",
    "    lora5_model = PeftModel.from_pretrained(\n",
    "        AutoModelForCausalLM.from_pretrained(model_name),\n",
    "        \"lora-5epoch\"\n",
    "    )\n",
    "    lora5_answer = infer(lora5_model, test_question)\n",
    "\n",
    "    # 3. LoRA 微调模型（10轮）\n",
    "    lora10_model = PeftModel.from_pretrained(\n",
    "        AutoModelForCausalLM.from_pretrained(model_name),\n",
    "        \"lora-10epoch\"\n",
    "    )\n",
    "    lora10_answer = infer(lora10_model, test_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 3：微调效果分析\n",
    "\n",
    "#### 问题 5：训练前后回答对比\n",
    "- 用原始模型和5轮训练后的模型，回答同一问题，对比结果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "问题: 请简述LoRA的基本思想。\n",
      "==================================================\n",
      "\n",
      "【原始Qwen模型回答】\n",
      "请简述LoRA的基本思想。LoRA的基本思想是：（1）在LoRA中，每个用户都拥有一个唯一的ID，ID是唯一的，且不可更改；（2）每个用户都拥有一个唯一的IP地址，IP地址是唯一的，且不可更改；（3）每个用户都拥有一个唯一的MAC地址，MAC地址是唯一的，且不可更改；（4）每个用户都拥有一个唯一的IP地址和MAC地址，IP地址和MAC地址是不可更改的；（5）每个用户都拥有一个唯一的ID，ID是唯一的，且不可更改；（6）每个用户都拥有一个唯一的IP地址和\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "【LoRA微调后（10轮）回答】\n",
      "请简述LoRA的基本思想。 LoRA的基本思想是将所有交易都记录在一张票据上，然后将票据与票据进行比较，以确定交易是否合法。这种做法可以确保交易的合法性，并且可以防止欺诈行为。LoRA还允许交易者在票据上添加其他信息，例如交易日期、交易金额、交易地点等。这种做法可以提高交易者的信任度，并且可以防止欺诈行为。LoRA还允许交易者在票据上添加其他信息，例如交易日期、交易金额、交易地点等。这种做法可以提高交易者的信任度，并且可以防止欺诈行为。\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 问题5：训练前后对比\n",
    "# 问题5：训练前后对比 - 直接查看文本输出\n",
    "test_question = \"请简述LoRA的基本思想。\"\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"问题: {test_question}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n【原始Qwen模型回答】\")\n",
    "print(orig_answer)\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "print(\"【LoRA微调后（10轮）回答】\")\n",
    "print(lora5_answer)\n",
    "print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "问题: 请简述LoRA的基本思想。\n",
      "==================================================\n",
      "\n",
      "【LoRA微调后（5轮）回答】\n",
      "请简述LoRA的基本思想。 LoRA的基本思想是将所有交易都记录在一张票据上，然后将票据与票据进行比较，以确定交易是否合法。这种做法可以确保交易的合法性，并且可以防止欺诈行为。LoRA还允许交易者在票据上添加其他信息，例如交易日期、交易金额、交易地点等。这种做法可以提高交易者的信任度，并且可以防止欺诈行为。LoRA还允许交易者在票据上添加其他信息，例如交易日期、交易金额、交易地点等。这种做法可以提高交易者的信任度，并且可以防止欺诈行为。\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "【LoRA微调后（10轮）回答】\n",
      "请简述LoRA的基本思想。 LoRA的基本思想是将所有业务流程视为一个整体，将每个业务流程视为一个独立的实体。这个实体可以是任何实体，包括客户、供应商、合作伙伴、员工、政府机构等。LoRA 的核心思想是将业务流程视为一个整体，以实现业务流程的自动化和标准化。通过使用自动化工具和技术，LoRA 可以实现业务流程的自动化和标准化，从而提高效率和降低成本。LoRA 的另一个核心思想是将业务流程视为一个整体，以实现业务流程的标准化和自动化。通过使用自动化工具和技术，LoRA 可以实现业务流程的\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 问题6：5轮和10轮对比 - 直接查看文本输出\n",
    "test_question = \"请简述LoRA的基本思想。\"\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"问题: {test_question}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n【LoRA微调后（5轮）回答】\")\n",
    "print(lora5_answer)\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "print(\"【LoRA微调后（10轮）回答】\")\n",
    "print(lora10_answer)\n",
    "print(\"=\"*50) # 问题6：5轮和10轮对比\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
